{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83e63c9-45e5-4465-b35c-b49d7cf4fc9c",
   "metadata": {},
   "source": [
    "# Multimodal Cancer Modeling in the Age of Foundation Model Embeddings\n",
    "### Steven Song\\*, Morgan Borjigin-Wang\\*, Irene R. Madejski, Robert L. Grossman\n",
    "\n",
    "\\* Equal contribution\n",
    "\n",
    "Read our paper here: https://arxiv.org/abs/2505.07683\n",
    "\n",
    "***\n",
    "\n",
    "The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference dataset in cancer through its harmonized genomics, clinical, and imaging data. Numerous prior studies have developed bespoke deep learning models over TCGA for tasks such as cancer survival prediction. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive feature embeddings agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized.\n",
    "\n",
    "* **We investigate the ability to train classical machine learning models over multimodal, zero-shot FM embeddings of cancer data.**\n",
    "* We demonstrate the ease and additive effect of multimodal fusion, outperforming unimodal models.\n",
    "* Overall, we propose a simple, modernized approach to multimodal cancer modeling using FM embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f6501-b5f7-4ae0-9a27-12376c263321",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/StevenSong/multimodal-cancer-modeling/refs/heads/main/overview.png\" alt=\"conceptual overview figure\" width=\"50%\"/>\n",
    "\n",
    "Conceptually, the proposed framework does late fusion of unimodal models trained over their respective embeddings. Specifically, we use:\n",
    "* BulkRNABert ([Gélard et al. 2025)](https://proceedings.mlr.press/v259/gelard25a.html)) for RNA-seq data\n",
    "* UNI2-h ([Chen et al. 2024](https://www.nature.com/articles/s41591-024-02857-3)) for histology data\n",
    "* BioMistral ([Labrak et al. 2024](https://aclanthology.org/2024.findings-acl.348/)) for pathology report data (summarized by Llama-3.1-8B-Instruct ([Grattafiori et al. 2024](https://arxiv.org/abs/2407.21783)))\n",
    "\n",
    "We can specifically break down the required steps for our approach as follows:\n",
    "1. Get TCGA embeddings\n",
    "1. Get TCGA metadata\n",
    "1. Merge embeddings and metadata\n",
    "1. Prepare experiments\n",
    "1. Train unimodal models\n",
    "1. Train multimodal model\n",
    "1. Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Survival Experiments\n",
    "\n",
    "This notebook contains all our code for survival modeling.\n",
    "\n",
    "The experiments test multimodal fusion of survival models and varying dimensionality reductions for high-dimensional embeddings.\n",
    "\n",
    "Specifically, we experiment with 5 modalities:\n",
    "* Patient demographics (sex, age - binned, race, ethnicity)\n",
    "* Cancer type (we use the TCGA project ID as a proxy for cancer type)\n",
    "* RNA-seq gene expression (`BulkRNABert` embeddings)\n",
    "* Whole slide histology images (`UNI2` embeddings)\n",
    "* Pathology reports (`BioMistral` embeddings)\n",
    "\n",
    "We additionally experiment with various alternate embeddings, including:\n",
    "* `BioMistral` embeddings of pathology report summaries generated by `Llama-3.1-8B-Instruct`\n",
    "* `Mistral-7B-Instruct-v0.1` embeddings of pathology reports\n",
    "* `Mistral-7B-Instruct-v0.1` embeddings of pathology report summaries generated by `Llama-3.1-8B-Instruct`\n",
    "* `UCE` embeddings of RNA-seq gene expression\n",
    "\n",
    "To use these alternate embeddings, modify the variables for input/output files in the first code cell of this notebook.\n",
    "\n",
    "Run experiments by executing all cells of this notebook. Results are saved in the `results` subdirectory at the root of the repo. Analysis and visualization is done using tools also in the `results` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_file = \"expr.h5\" # BulkRNABert\n",
    "hist_file = \"hist.h5\" # UNI2\n",
    "text_file = \"summ.h5\" # BioMistral - Summarized\n",
    "output_results = \"results/results_summarized.csv\"\n",
    "output_predictions = \"results/predictions_summarized.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clinical.csv\")\n",
    "clin_case_ids = set(df[\"case_id\"])\n",
    "\n",
    "with h5py.File(expr_file, \"r\") as expr_h5:\n",
    "    expr_case_ids = set(expr_h5.keys())\n",
    "\n",
    "with h5py.File(hist_file, \"r\") as hist_h5:\n",
    "    hist_case_ids = set(hist_h5.keys())\n",
    "\n",
    "with h5py.File(text_file, \"r\") as text_h5:\n",
    "    text_case_ids = set(text_h5.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ids = sorted(list(clin_case_ids & expr_case_ids & hist_case_ids & text_case_ids))\n",
    "\n",
    "df = df[df[\"case_id\"].isin(case_ids)]\n",
    "df = df.sort_values(\"case_id\").reset_index(drop=True)\n",
    "assert df[\"case_id\"].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5323d814-ec0b-4627-ba45-25b0c0813262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7982, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age_binned\"] = pd.cut(\n",
    "    df[\"age\"],\n",
    "    bins=[0, 20, 40, 60, 80, 100],\n",
    "    labels=[\"(0, 20]\", \"(20, 40]\", \"(40, 60]\", \"(60, 80]\", \"(80, 100]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = df[\"vital_status\"] == \"Dead\"\n",
    "days_to_event = np.where(dead, df[\"days_to_death\"], df[\"days_to_last_follow_up\"])\n",
    "assert not np.isnan(days_to_event).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(zip(dead, days_to_event)), dtype=[('Status', '?'), ('Survival_in_days', '<f8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ohe = OneHotEncoder(drop=\"if_binary\", sparse_output=False, dtype=np.float32)\n",
    "canc_ohe = OneHotEncoder(drop=\"if_binary\", sparse_output=False, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_X = demo_ohe.fit_transform(df[[\"sex\", \"age_binned\", \"race\", \"ethnicity\"]])\n",
    "canc_X = canc_ohe.fit_transform(df[[\"project\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7982, 17)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7982, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canc_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['female', 'male'], dtype=object),\n",
       " array(['(0, 20]', '(20, 40]', '(40, 60]', '(60, 80]', '(80, 100]'],\n",
       "       dtype=object),\n",
       " array(['Unknown', 'american indian or alaska native', 'asian',\n",
       "        'black or african american',\n",
       "        'native hawaiian or other pacific islander', 'not reported',\n",
       "        'white'], dtype=object),\n",
       " array(['Unknown', 'hispanic or latino', 'not hispanic or latino',\n",
       "        'not reported'], dtype=object)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['TCGA-ACC', 'TCGA-BLCA', 'TCGA-BRCA', 'TCGA-CESC', 'TCGA-CHOL',\n",
       "        'TCGA-COAD', 'TCGA-DLBC', 'TCGA-ESCA', 'TCGA-GBM', 'TCGA-HNSC',\n",
       "        'TCGA-KICH', 'TCGA-KIRC', 'TCGA-KIRP', 'TCGA-LGG', 'TCGA-LIHC',\n",
       "        'TCGA-LUAD', 'TCGA-LUSC', 'TCGA-MESO', 'TCGA-OV', 'TCGA-PAAD',\n",
       "        'TCGA-PCPG', 'TCGA-PRAD', 'TCGA-READ', 'TCGA-SARC', 'TCGA-SKCM',\n",
       "        'TCGA-STAD', 'TCGA-TGCT', 'TCGA-THCA', 'TCGA-THYM', 'TCGA-UCEC',\n",
       "        'TCGA-UCS', 'TCGA-UVM'], dtype=object)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canc_ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_case_emb_from_h5(case_ids: list[str], h5: h5py.File):\n",
    "    X = []\n",
    "    for case_id in tqdm(case_ids):\n",
    "        case_group = h5[case_id]\n",
    "        embs = np.stack([v[:] for v in case_group.values()], axis=0)\n",
    "        emb = np.mean(embs, axis=0)\n",
    "        X.append(emb)\n",
    "    return np.stack(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7982/7982 [00:02<00:00, 2715.79it/s]\n",
      "100%|██████████| 7982/7982 [00:05<00:00, 1460.10it/s]\n",
      "100%|██████████| 7982/7982 [00:07<00:00, 1033.27it/s]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(expr_file, \"r\") as expr_h5:\n",
    "    expr_X = extract_case_emb_from_h5(case_ids, expr_h5)\n",
    "\n",
    "with h5py.File(hist_file, \"r\") as hist_h5:\n",
    "    hist_X = extract_case_emb_from_h5(case_ids, hist_h5)\n",
    "\n",
    "with h5py.File(text_file, \"r\") as text_h5:\n",
    "    text_X = extract_case_emb_from_h5(case_ids, text_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "splitter = (\n",
    "    df[\"vital_status\"]\n",
    "    + \"_\"\n",
    "    + df[\"project\"]\n",
    "    + \"_\"\n",
    "    + df[\"sex\"]\n",
    "    + \"_\"\n",
    "    + df[\"age_binned\"].astype(str)\n",
    "    + \"_\"\n",
    "    + df[\"race\"]\n",
    "    + \"_\"\n",
    "    + df[\"ethnicity\"]\n",
    ")\n",
    "\n",
    "n = len(df)\n",
    "test_splits = [split_idxs for _, split_idxs in skf.split(X=np.zeros(n), y=splitter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc3eb583-8b87-4780-9aec-f3ad1fdabc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Alive_TCGA-LUAD_male_(60, 80]_not reported_not...\n",
       "1       Alive_TCGA-LUAD_male_(60, 80]_not reported_not...\n",
       "2       Dead_TCGA-LUAD_female_(60, 80]_not reported_no...\n",
       "3       Alive_TCGA-LUAD_male_(60, 80]_not reported_not...\n",
       "4       Dead_TCGA-LUAD_male_(60, 80]_not reported_not ...\n",
       "                              ...                        \n",
       "7977    Alive_TCGA-LIHC_female_(60, 80]_white_not hisp...\n",
       "7978    Alive_TCGA-LIHC_male_(40, 60]_white_not hispan...\n",
       "7979    Alive_TCGA-THYM_female_(60, 80]_white_not hisp...\n",
       "7980    Dead_TCGA-CHOL_male_(40, 60]_white_not hispani...\n",
       "7981    Alive_TCGA-CESC_female_(60, 80]_white_not hisp...\n",
       "Length: 7982, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = df[[\"case_id\"]].copy()\n",
    "meta_df[\"split\"] = -1\n",
    "meta_df[\"split_order\"] = -1\n",
    "for i, test_idxs in enumerate(test_splits):\n",
    "    meta_df.loc[test_idxs, \"split\"] = i\n",
    "    meta_df.loc[test_idxs, \"split_order\"] = list(range(len(test_idxs)))\n",
    "meta_df[\"dead\"] = y[\"Status\"]\n",
    "meta_df[\"days_to_death_or_censor\"] = y[\"Survival_in_days\"]\n",
    "meta_df.to_csv(\"results/split_cases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "0    1597\n",
       "1    1597\n",
       "3    1596\n",
       "4    1596\n",
       "2    1596\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_split(\n",
    "    *,  # enforce kwargs\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    pca_components: int or None,\n",
    "    standardize: bool,\n",
    "    name: str = \"\",\n",
    "    verbose: bool = False,\n",
    ") -> dict:\n",
    "    if verbose:\n",
    "        print(f\"Running {name}\")\n",
    "\n",
    "    # z-score input features\n",
    "    if standardize:\n",
    "        if verbose:\n",
    "            print(\"--standardized\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    # dimensionality reduction\n",
    "    if pca_components is not None:\n",
    "        if verbose:\n",
    "            print(\"--reduced\")\n",
    "        pca = PCA(n_components=pca_components, random_state=42)\n",
    "        X_train_red = pca.fit_transform(X_train_scaled)\n",
    "        X_test_red = pca.transform(X_test_scaled)\n",
    "    else:\n",
    "        X_train_red = X_train_scaled\n",
    "        X_test_red = X_test_scaled\n",
    "\n",
    "    # fit survival model\n",
    "    cox = CoxPHSurvivalAnalysis(alpha=0.1).fit(X_train_red, y_train)\n",
    "\n",
    "    # generate predictions\n",
    "    y_train_pred = cox.predict(X_train_red)\n",
    "    y_test_pred = cox.predict(X_test_red)\n",
    "\n",
    "    # evaluate predictions\n",
    "    c_index = concordance_index_censored(\n",
    "        event_indicator=y_test[\"Status\"],\n",
    "        event_time=y_test[\"Survival_in_days\"],\n",
    "        estimate=y_test_pred,\n",
    "    )[0]\n",
    "\n",
    "    return {\n",
    "        \"c_index\": c_index,\n",
    "        \"y_test_pred\": y_test_pred,\n",
    "        \"y_train_pred\": y_train_pred,\n",
    "    }\n",
    "\n",
    "def run_unimodal_split(\n",
    "    *,  # enforce kwargs\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    test_idxs: np.ndarray,\n",
    "    train_idxs: np.ndarray,\n",
    "    pca_components: int or None,\n",
    "    standardize: bool,\n",
    "    name: str = \"\",\n",
    "    verbose: bool = False,\n",
    ") -> dict:\n",
    "    # split matrices\n",
    "    X_train, X_test = X[train_idxs], X[test_idxs]\n",
    "    y_train, y_test = y[train_idxs], y[test_idxs]\n",
    "\n",
    "    return run_split(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        pca_components=pca_components,\n",
    "        standardize=standardize,\n",
    "        name=name,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "def powerset(s):\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(pca_components: int) -> dict:\n",
    "    results = []\n",
    "    for test_idxs in tqdm(test_splits, desc=\"Cross Validation Splits\"):\n",
    "        split_results = dict()\n",
    "\n",
    "        temp = set(test_idxs)\n",
    "        train_idxs = [i for i in range(n) if i not in temp]\n",
    "\n",
    "        split_results[\"demo\"] = run_unimodal_split(X=demo_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=None, standardize=False)\n",
    "        split_results[\"canc\"] = run_unimodal_split(X=canc_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=None, standardize=False)\n",
    "        split_results[\"expr\"] = run_unimodal_split(X=expr_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_components, standardize=True)\n",
    "        split_results[\"hist\"] = run_unimodal_split(X=hist_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_components, standardize=True)\n",
    "        split_results[\"text\"] = run_unimodal_split(X=text_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_components, standardize=True)\n",
    "\n",
    "        y_train, y_test = y[train_idxs], y[test_idxs]\n",
    "\n",
    "        combos = [sorted(x) for x in powerset([\"demo\", \"canc\", \"expr\", \"hist\", \"text\"]) if len(x) > 1]\n",
    "        for combo in combos:\n",
    "            mult_X_train = []\n",
    "            mult_X_test = []\n",
    "            for modality in combo:\n",
    "                x_train = split_results[modality][\"y_train_pred\"][:, np.newaxis]\n",
    "                x_test = split_results[modality][\"y_test_pred\"][:, np.newaxis]\n",
    "                # z-score all unimodal risks\n",
    "                scaler = StandardScaler()\n",
    "                x_train = scaler.fit_transform(x_train)\n",
    "                x_test = scaler.transform(x_test)\n",
    "                mult_X_train.append(x_train)\n",
    "                mult_X_test.append(x_test)\n",
    "\n",
    "            mult_X_train = np.concatenate(mult_X_train, axis=1)\n",
    "            mult_X_test = np.concatenate(mult_X_test, axis=1)\n",
    "\n",
    "            split_results[\"-\".join(combo)] = run_split(X_train=mult_X_train, y_train=y_train, X_test=mult_X_test, y_test=y_test, pca_components=None, standardize=False)\n",
    "\n",
    "        results.append(split_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross Validation Splits: 100%|██████████| 5/5 [03:38<00:00, 43.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# run experiments for 256 components based on paper\n",
    "results = dict()\n",
    "pca_components = 256\n",
    "results[pca_components] = run_experiment(pca_components=pca_components)\n",
    "np.save(output_predictions, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44adabbb-4c34-4822-9078-2603f34e3813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canc-demo-expr-hist-text\n"
     ]
    }
   ],
   "source": [
    "indiv_combos = [\"canc\", \"demo\", \"expr\", \"hist\", \"text\"]\n",
    "fusion = '-'.join(indiv_combos)\n",
    "print(fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d40a780-7692-4277-9f66-4e46694814f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unimodal Results\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3855.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canc:0.7370195470669764\n",
      "demo:0.6298858244261268\n",
      "expr:0.7495909587116658\n",
      "hist:0.7539801485317722\n",
      "text:0.7519210591393961\n",
      "Multimodal Results\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8905.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canc-demo-expr-hist-text:0.7932592520350699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Unimodal Results\")\n",
    "print(\"------------------\")\n",
    "for pca_components in tqdm([256]):\n",
    "    for combo in indiv_combos:\n",
    "        c_idxs = []\n",
    "        for i in range(5):\n",
    "            c_idx = results[pca_components][i][combo][\"c_index\"]\n",
    "            c_idxs.append(c_idx)\n",
    "        c_idx = np.mean(c_idxs)\n",
    "        print(f'{combo}:{c_idx}')\n",
    "\n",
    "print(\"Multimodal Results\")\n",
    "print(\"------------------\")\n",
    "for pca_components in tqdm([256]):\n",
    "    for combo in [fusion]:\n",
    "        c_idxs = []\n",
    "        for i in range(5):\n",
    "            c_idx = results[pca_components][i][combo][\"c_index\"]\n",
    "            c_idxs.append(c_idx)\n",
    "        c_idx = np.mean(c_idxs)\n",
    "        print(f'{combo}:{c_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
